{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this derivation, we introduce a method called **Probabilistic Localized Backpropagation**, where backpropagation is localized along specific paths selected probabilistically. This approach aims to focus the learning process on the most significant pathways contributing to the network's output error, potentially improving computational efficiency and training effectiveness.\n",
    "\n",
    "## Probabilistic Path Selection Method\n",
    "\n",
    "The selection of paths through the network is based on both the error at the output layer and the magnitudes of the weights connecting neurons in successive layers. Here's how the method works:\n",
    "\n",
    "1. **Compute Error at the Output Layer:**\n",
    "\n",
    "   - For each sample $x$ in the batch, compute the absolute error between the network's output $a^L(x)$ and the target output $y(x)$:\n",
    "     $$\n",
    "     \\text{error}(x) = |a^L(x) - y(x)|\n",
    "     $$\n",
    "   - Aggregate the error over all samples to get the total error for each output neuron:\n",
    "     $$\n",
    "     \\text{aggregated\\_error} = \\sum_x \\text{error}(x)\n",
    "     $$\n",
    "\n",
    "2. **Compute Probabilities for Output Neurons:**\n",
    "\n",
    "   - Calculate the probability of selecting each output neuron, proportional to its aggregated error:\n",
    "     $$\n",
    "     P_{\\text{output\\_neuron}_j} = \\frac{\\text{aggregated\\_error}_j}{\\sum_j \\text{aggregated\\_error}_j}\n",
    "     $$\n",
    "   - Neurons with higher errors have a higher chance of being selected.\n",
    "\n",
    "3. **Select Output Neurons Probabilistically:**\n",
    "\n",
    "   - Randomly select an output neuron $o^L$ as the starting point for a path, based on the probabilities $P_{\\text{output\\_neuron}_j}$.\n",
    "\n",
    "4. **Traverse Backward Through the Network:**\n",
    "\n",
    "   - For each layer $l$ from the output layer $L$ down to the first hidden layer:\n",
    "     - Consider the weights connecting neurons from layer $l-1$ to the selected neuron $o^l$ in layer $l$.\n",
    "     - Compute the probabilities for the neurons in layer $l-1$ proportional to the absolute values of these weights:\n",
    "       $$\n",
    "       P_{\\text{neuron}_k}^{(l-1)} = \\frac{|w^l_{o^l k}|}{\\sum_k |w^l_{o^l k}|}\n",
    "       $$\n",
    "     - Randomly select a neuron $o^{l-1}$ in layer $l-1$ based on these probabilities.\n",
    "     - Add the connection $(o^{l-1}, o^l)$ to the path.\n",
    "     - Set $o^l \\leftarrow o^{l-1}$ and proceed to the next layer.\n",
    "\n",
    "5. **Repeat for Multiple Paths:**\n",
    "\n",
    "   - Repeat steps 3 and 4 to select multiple paths (e.g., $n_{\\text{paths}} = 5$).\n",
    "\n",
    "## Mathematical Derivation\n",
    "\n",
    "With this probabilistic path selection method, the standard backpropagation equations are modified to account for the localization. Let's revisit key components of the derivation, incorporating the path selection:\n",
    "\n",
    "### 1. Feedforward Equations\n",
    "\n",
    "- **Weighted Input:**\n",
    "  $$\n",
    "  z^l_j = \\sum_k w^l_{jk} a^{l-1}_k + b^l_j\n",
    "  $$\n",
    "- **Activation:**\n",
    "  $$\n",
    "  a^l_j = \\sigma(z^l_j)\n",
    "  $$\n",
    "\n",
    "### 2. Cost Function\n",
    "\n",
    "- **Mean Squared Error (MSE):**\n",
    "  $$\n",
    "  C = \\frac{1}{2n} \\sum_x \\| y(x) - a^L(x) \\|^2\n",
    "  $$\n",
    "  - $n$ is the number of samples.\n",
    "  - $y(x)$ is the target output.\n",
    "  - $a^L(x)$ is the network's output.\n",
    "\n",
    "### 3. Gradient Descent Update Rules\n",
    "\n",
    "- **Weights Update:**\n",
    "  $$\n",
    "  w^l_{jk} \\leftarrow w^l_{jk} - \\eta \\frac{\\partial C}{\\partial w^l_{jk}}\n",
    "  $$\n",
    "- **Biases Update:**\n",
    "  $$\n",
    "  b^l_j \\leftarrow b^l_j - \\eta \\frac{\\partial C}{\\partial b^l_j}\n",
    "  $$\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "### 4. Error Term $\\delta^l_j$\n",
    "\n",
    "- **General Definition:**\n",
    "  $$\n",
    "  \\delta^l_j = \\frac{\\partial C}{\\partial z^l_j}\n",
    "  $$\n",
    "- **Localized Error Propagation:**\n",
    "  - For neurons **not** on the selected paths:\n",
    "    $$\n",
    "    \\delta^l_j = 0 \\quad \\text{if} \\quad j \\notin \\{ o^l \\}\n",
    "    $$\n",
    "  - For neurons **on** the selected paths:\n",
    "    $$\n",
    "    \\delta^l_{o^l} = \\frac{\\partial C}{\\partial z^l_{o^l}}\n",
    "    $$\n",
    "\n",
    "### 5. Backpropagation Equations for Neurons on the Path\n",
    "\n",
    "#### Error Term Recursion\n",
    "\n",
    "For neurons on the selected path:\n",
    "\n",
    "$$\n",
    "\\delta^l_{o^l} = \\left( w^{l+1}_{o^{l+1} o^l} \\delta^{l+1}_{o^{l+1}} \\right) \\sigma'(z^l_{o^l})\n",
    "$$\n",
    "\n",
    "- $w^{l+1}_{o^{l+1} o^l}$ is the weight connecting neuron $o^l$ in layer $l$ to neuron $o^{l+1}$ in layer $l+1$.\n",
    "- $\\delta^{l+1}_{o^{l+1}}$ is the error term for the neuron in the next layer.\n",
    "- $\\sigma'$ is the derivative of the activation function.\n",
    "\n",
    "#### Output Layer Error Term\n",
    "\n",
    "For the selected output neuron $o^L$:\n",
    "\n",
    "$$\n",
    "\\delta^L_{o^L} = \\left( a^L_{o^L}(x) - y_{o^L}(x) \\right) \\sigma'(z^L_{o^L})\n",
    "$$\n",
    "\n",
    "- $a^L_{o^L}(x)$ is the activation of the selected output neuron.\n",
    "- $y_{o^L}(x)$ is the target value for the selected output neuron.\n",
    "\n",
    "### 6. Gradients with Respect to Weights and Biases\n",
    "\n",
    "#### Biases\n",
    "\n",
    "For biases of neurons on the path:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^l_{o^l}} = \\delta^l_{o^l}\n",
    "$$\n",
    "\n",
    "#### Weights\n",
    "\n",
    "For weights connecting neurons on the path:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^l_{o^l o^{l-1}}} = a^{l-1}_{o^{l-1}} \\delta^l_{o^l}\n",
    "$$\n",
    "\n",
    "- $a^{l-1}_{o^{l-1}}$ is the activation of the neuron $o^{l-1}$ in layer $l-1$.\n",
    "\n",
    "### 7. Implications for Backpropagation\n",
    "\n",
    "By selecting paths probabilistically based on the output error and weight magnitudes, the backpropagation process focuses on the most significant pathways contributing to the network's overall error.\n",
    "\n",
    "- **Localized Error Propagation:**\n",
    "  - Only neurons and weights along the selected paths are involved in the error propagation and weight updates.\n",
    "- **Computational Efficiency:**\n",
    "  - Reduces computational load by limiting calculations to a subset of the network.\n",
    "- **Dynamic Focus:**\n",
    "  - The selection process adapts over time as the network's outputs and weights change.\n",
    "\n",
    "## Correctness of the Derivation\n",
    "\n",
    "The derivation remains mathematically consistent under this probabilistic localized approach. However, there are important considerations:\n",
    "\n",
    "- **Selective Backpropagation:**\n",
    "  - By zeroing out $\\delta^l_j$ for neurons not on the selected paths, the gradient descent updates are applied only to a subset of weights and biases.\n",
    "- **Expectation over Multiple Paths:**\n",
    "  - Over multiple iterations and paths, the stochastic updates can approximate the full gradient descent, especially if paths are selected in a way that covers the network adequately over time.\n",
    "- **Variance in Updates:**\n",
    "  - The stochastic nature introduces variance in the updates, which may affect convergence rates and stability.\n",
    "\n",
    "## Benefits and Trade-offs\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Efficiency:**\n",
    "  - Reduces computational complexity, making it suitable for large-scale networks.\n",
    "- **Focus on Significant Errors:**\n",
    "  - Prioritizes correcting the largest errors.\n",
    "- **Adaptability:**\n",
    "  - Dynamically adjusts which paths to focus on as errors and weights change.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Incomplete Gradient Information:**\n",
    "  - May miss important updates from neurons not on the selected paths.\n",
    "- **Convergence Concerns:**\n",
    "  - Requires careful consideration to ensure convergence to a good solution.\n",
    "- **Hyperparameter Sensitivity:**\n",
    "  - The number of paths $n_{\\text{paths}}$ and the methods for computing selection probabilities can significantly impact performance.\n",
    "\n",
    "## Empirical Validation\n",
    "\n",
    "To ensure that this method is effective:\n",
    "\n",
    "- **Experimentation:**\n",
    "  - Conduct experiments comparing the probabilistic localized backpropagation with standard backpropagation.\n",
    "- **Performance Metrics:**\n",
    "  - Evaluate metrics such as training time, convergence rate, and final accuracy.\n",
    "- **Parameter Tuning:**\n",
    "  - Explore the impact of varying $n_{\\text{paths}}$ and the methods for computing selection probabilities.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By integrating the probabilistic path selection method into the backpropagation derivation, we develop a framework for localized backpropagation that leverages the network's error and weight structures. This approach can potentially improve training efficiency and focus learning on the most impactful network components. The mathematical derivation remains correct under this framework, provided that the probabilistic selections are properly managed and the implications for gradient approximation are considered.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
